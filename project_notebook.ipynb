{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_notebook.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNepMn5SYw1L0XtDlq2nrH3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Birkbeck/bsc-computer-science-project-2021_22-mohammadreza490/blob/main/project_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setting up paths and import libraries"
      ],
      "metadata": {
        "id": "IEmIKUzXoggs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the following variables\n",
        "PATH_TO_SRC_FOLDER = None\n",
        "PATH_TO_PROJECT_FOLDER = None"
      ],
      "metadata": {
        "id": "FMxRpf6Wnvwv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if you stored the unzipped file in google drive, you can mount your drive in colab at /content/gdrive like below\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZWOZcv4sXMM",
        "outputId": "208c2c05-fedf-4cbb-85a8-679008249762"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSOeC37OnHXK"
      },
      "outputs": [],
      "source": [
        "!pip install musdb\n",
        "!pip install pydub\n",
        "!pip install tensorflow-addons\n",
        "!pip install ipdb\n",
        "!pip install museval\n",
        "!pip install librosa\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import IPython.display as ipd\n",
        "import musdb\n",
        "import subprocess\n",
        "import pydub\n",
        "import tensorflow_addons as tfa\n",
        "import time\n",
        "import shutil\n",
        "import ipdb\n",
        "import scipy as sp\n",
        "import soundfile as sf\n",
        "import museval\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload"
      ],
      "metadata": {
        "id": "6S7iqkA1nKl9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%autoreload 2"
      ],
      "metadata": {
        "id": "Actc-G2PnOYd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "if PATH_TO_SRC_FOLDER not in sys.path:\n",
        "  sys.path.append(PATH_TO_SRC_FOLDER) #https://stackoverflow.com/questions/48905127/importing-py-files-in-google-colab"
      ],
      "metadata": {
        "id": "eI40WAOunVTt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from config_handler import Config_Handler\n",
        "from dataset_handler import Dataset_Handler\n",
        "from model_architecture_builder import Model_Architecture_Builder\n",
        "from model_handler import Model_Handler\n",
        "from printer import Printer\n",
        "from visualiser import Visualiser\n",
        "from wav_file_handler import Wav_File_Handler"
      ],
      "metadata": {
        "id": "_rDTSrS_nQfa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Config_Handler.init(PATH_TO_PROJECT_FOLDER)"
      ],
      "metadata": {
        "id": "vefBYE80n47a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training a model (same as model_shfl_vocal_mdl6)\n"
      ],
      "metadata": {
        "id": "SouNPFH4pFmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(y_true:np.array, y_pred:np.array, alpha:float=1.0)->float:\n",
        "      '''\n",
        "      this returns the customised loss function as mentioned in the paper\n",
        "      the result will be calculated as loss(Singing Voice) = alpha ∗ L(vocal, channelvocal) +(1 − alpha) ∗ L(acc, channelacc)\n",
        "      where L() is the average of L1 losses on every pixel. with alpha = 1.0, we only get the vocals and not the accompaniments\n",
        "      '''\n",
        "      vocal_loss = tf.reduce_mean(tf.abs(y_true[..., 0] - y_pred[..., 0]))\n",
        "      accompaniment_loss = tf.reduce_mean(tf.abs(y_true[..., 1] - y_pred[..., 1]))\n",
        "      #L1 loss function: https://afteracademy.com/blog/what-are-l1-and-l2-loss-functions\n",
        "      return alpha * vocal_loss + (1 - alpha) * accompaniment_loss\n",
        "\n",
        "def learning_rate_scheduler(epoch:int, lr:float)->float:\n",
        "     if epoch == 20:\n",
        "       lr = 1e-4 \n",
        "     return lr\n",
        "\n",
        "def data_generator(model:Model_Handler, batch_size:int=8):\n",
        "        path_to_dir = Config_Handler.PATH_TO_TRAIN_DATA_DIR()\n",
        "        available_musics =  os.listdir(path_to_dir)\n",
        "        if f\"{model._model_name}-spectrograms\" in available_musics:\n",
        "            available_musics.remove(f\"{model._model_name}-spectrograms\")\n",
        "        if \"model_9_vocal_with_model_6_data_generator-spectrograms\" in available_musics:\n",
        "            available_musics.remove(\"model_9_vocal_with_model_6_data_generator-spectrograms\")\n",
        "        music_dict = {}\n",
        "        for music_name in available_musics:\n",
        "            \n",
        "            path = os.path.join(Config_Handler.PATH_TO_TEMP_SPECTROGRAMS_FOR_TRAINING_DIR(), music_name)\n",
        "            number_of_available_segments = Wav_File_Handler(audio_path = os.path.join(Config_Handler.PATH_TO_TRAIN_DATA_DIR(), music_name, \"mixture.wav\")).get_number_of_possible_segments()\n",
        "            music_dict[music_name] = {\"available_spectrograms_ids\": [str(spec_id) for spec_id in range(number_of_available_segments)]}\n",
        "        batch_X = []\n",
        "        batch_y = {\"vocal_spectrograms\" : [],\n",
        "        \"ac_spectrograms\": []}\n",
        "        \n",
        "        while len(available_musics) > 0:\n",
        "            np.random.shuffle(available_musics)\n",
        "            for music_name in reversed(available_musics):\n",
        "                spec_dir_path = os.path.join(Config_Handler.PATH_TO_TEMP_SPECTROGRAMS_FOR_TRAINING_DIR(), music_name)\n",
        "                specs = np.load(os.path.join(spec_dir_path, \"spectrograms.npy\"), allow_pickle=True).item()\n",
        "                while len(music_dict[music_name][\"available_spectrograms_ids\"]) > 0:\n",
        "                    np.random.shuffle(music_dict[music_name][\"available_spectrograms_ids\"])\n",
        "                    if len(music_dict[music_name][\"available_spectrograms_ids\"]) < batch_size:\n",
        "                        spectrogram_ids_to_select = music_dict[music_name][\"available_spectrograms_ids\"]\n",
        "                    else:\n",
        "                        spectrogram_ids_to_select = music_dict[music_name][\"available_spectrograms_ids\"][:batch_size]\n",
        "                    segments_ids_to_select = [spec_id for spec_id in spectrogram_ids_to_select]\n",
        "                    music_dict[music_name][\"available_spectrograms_ids\"] = music_dict[music_name][\"available_spectrograms_ids\"][batch_size:]\n",
        "                    batch_X = np.array([specs[spec_id][\"mixture\"] for spec_id in segments_ids_to_select])\n",
        "                    batch_y[\"vocal_spectrograms\"] = np.array([specs[spec_id][\"vocals\"] for spec_id in segments_ids_to_select])\n",
        "                    batch_y[\"ac_spectrograms\"] = np.array([specs[spec_id][\"accompaniment\"] for spec_id in segments_ids_to_select])\n",
        "                    X = np.array(batch_X)\n",
        "                    y = np.array(np.stack([batch_y[\"vocal_spectrograms\"], batch_y[\"ac_spectrograms\"]], axis=-1))\n",
        "                    X = tf.squeeze(X)\n",
        "                    y = tf.squeeze(y)\n",
        "                    X = tf.expand_dims(X, -1) #this is for the input channel numbers (the input layer of cnn is has only one channel (look at the structure in the paper))\n",
        "                    y = tf.expand_dims(y, -1)\n",
        "                    if len(X.shape) == 3:\n",
        "                        #if for example there is only one element (one spectrogram), we add a batch size of one at the beggining\n",
        "                        X = tf.expand_dims(X, 0) #this is for the input channel numbers (the input layer of cnn is has only one channel (look at the structure in the paper))\n",
        "                        y = tf.expand_dims(y, 0)\n",
        "                    batch_X = []\n",
        "                    batch_y = {\"vocal_spectrograms\" : [],\n",
        "                \"ac_spectrograms\": []}\n",
        "                    yield(X, y)\n",
        "                    if len(music_dict[music_name][\"available_spectrograms_ids\"]) == 0:\n",
        "                        available_musics.remove(music_name)\n",
        "                    if len(available_musics) == 0:\n",
        "                        break "
      ],
      "metadata": {
        "id": "aRGiMKb8qji2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = Model_Handler(\"new_model\")"
      ],
      "metadata": {
        "id": "cMXghKnXqdd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model.train(loss_function, data_generator, learning_rate_scheduler)"
      ],
      "metadata": {
        "id": "cY5NdrLSrjj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load the pretrained model"
      ],
      "metadata": {
        "id": "4mpvxiXmob36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_shfl_vocal_mdl6 = Model_Handler(\"model_shfl_vocal_mdl6\") #loading the pretrained vocal model"
      ],
      "metadata": {
        "id": "Vq7pLAdboLnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to predict a song, either load it using librosa.load function or pass the full song path to the predict method\n",
        "\n",
        "v, a, o = model_shfl_vocal_mdl6.predict() #set either wav_array or audio_path here"
      ],
      "metadata": {
        "id": "MoSdhgQgooO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The four models trained in this project were coded like this:"
      ],
      "metadata": {
        "id": "YgORED_L2RKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_shfl_vocal_mdl6 = Model_Handler(\"model_shfl_vocal_mdl6\") #only vocals\n",
        "model_shfl_accompaniment_mdl6 = Model_Handler(\"model_shfl_accompaniment_mdl6\") #only accompaniments\n",
        "model_shfl_half_alpha_mdl6 = Model_Handler(\"model_shfl_half_alpha_mdl6\") #half vocals and half accompaniments\n",
        "model_shfl_more_vocals_mdl6 = Model_Handler(\"model_shfl_more_vocals_mdl6\") #more vocals"
      ],
      "metadata": {
        "id": "yXkEYcQ52ZFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_shfl_vocal_mdl6.train(loss_function, data_generator, learning_rate_scheduler)"
      ],
      "metadata": {
        "id": "R9AqI31N2vB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_shfl_accompaniment_mdl6.train(lambda y_true, y_pred: loss_function(y_true, y_pred, 0.0), data_generator, learning_rate_scheduler)"
      ],
      "metadata": {
        "id": "JDPU_uIa2wUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_shfl_half_alpha_mdl6.train(lambda y_true, y_pred: loss_function(y_true, y_pred, 0.50), data_generator, learning_rate_scheduler)"
      ],
      "metadata": {
        "id": "tbBp1wfo3AEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_shfl_more_vocals_mdl6.train(lambda y_true, y_pred: loss_function(y_true, y_pred, 0.707), data_generator, learning_rate_scheduler)"
      ],
      "metadata": {
        "id": "tF6TWumZ3Jvl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}